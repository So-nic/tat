{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "kl-FcYP6hcsf",
        "2mgTx5f7hj1S",
        "nCXiD3Kpidc6",
        "Zc_ZFoRPipeK",
        "RpLZfZ6Ts8uL",
        "2a3UpRIyjb07",
        "81Evw3y_khV3",
        "RpEL4s4ck_Bm",
        "eNJvLHoplcGL",
        "0b7JjNj4ltBz",
        "Ymdrfs4DnW16",
        "-3J2SN_joZDZ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Краткий экскурс по библиотекам"
      ],
      "metadata": {
        "id": "kl-FcYP6hcsf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TensorFlow** — библиотека сквозного машинного обучения Python для выполнения высококачественных численных вычислений. С помощью TensorFlow можно построить глубокие нейронные сети для распознавания образов и рукописного текста и рекуррентные нейронные сети для NLP(обработки естественных языков). Также есть модули для векторизации слов (embedding) и решения дифференциальных уравнений в частных производных (PDE). Этот фреймворк имеет отличную архитектурную поддержку, позволяющую с легкостью производить вычисления на самых разных платформах, в том числе на десктопах, серверах и мобильных устройствах."
      ],
      "metadata": {
        "id": "rsw5QmFQgzEQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Keras** — одна из основных библиотек Python с открытым исходным кодом, написанная для построения нейронных сетей и проектов машинного обучения. Keras может работать совместно с Deeplearning4j, MXNet, Microsoft Cognitive Toolkit (CNTK), Theano или TensorFlow. В этой библиотеке реализованы практически все автономные модули нейронной сети, включая оптимизаторы, нейронные слои, функции активации слоев, схемы инициализации, функции затрат и модели регуляризации. Это позволяет строить новые модули нейросети, просто добавляя функции или классы. И поскольку модель уже определена в коде, разработчику не приходится создавать для нее отдельные конфигурационные файлы.\n",
        "\n",
        "Keras особенно удобна для начинающих разработчиков, которые хотят проектировать и разрабатывать собственные нейронные сети. Также Keras можно использовать при работе со сверточными нейронными сетями. В нем реализованы алгоритмы нормализации, оптимизации и активации слоев. Keras не является ML-библиотекой полного цикла (то есть, исчерпывающей все возможные варианты построения нейронных сетей). Вместо этого она функционирует как очень дружелюбный, расширяемый интерфейс, увеличивающий модульность и выразительность (в том числе других библиотек)."
      ],
      "metadata": {
        "id": "wsT4SJ0Tg81b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Theano**\n",
        "\n",
        "По своей сути, это научная математическая библиотека, которая позволяет вам определять, оптимизировать и вычислять математические выражения, в том числе и в виде многомерных массивов. Основой большинства ML и AI приложений является многократное вычисление заковыристых математических выражений. Theano позволяет вам проводить подобные вычисления в сотни раз быстрее, вдобавок она отлично оптимизирована под GPU, имеет модуль для символьного дифференцирования, а также предлагает широкие возможности для тестирования кода.\n",
        "\n",
        "Когда речь идет о производительности, Theano — отличная библиотека ML и AI, поскольку она может работать с очень большими нейронными сетями. Ее целью является снижение времени разработки и увеличение скорости выполнения приложений, в частности, основанных на алгоритмах глубоких нейронных сетей. Ее единственный недостаток — не слишком простой синтаксис (по сравнению с TensorFlow), особенно для новичков."
      ],
      "metadata": {
        "id": "EBFxUVoZhEDK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Scikit-learn** — еще одна известная опенсорсная библиотека машинного обучения Python, с широким спектром алгоритмов кластеризации, регрессии и классификации. DBSCAN, градиентный бустинг, случайный лес, SVM и k-means — вот только несколько примеров. Она также отлично взаимодействует с другими научными библиотеками Python, такими как NumPy и SciPy.\n",
        "\n",
        "Эта библиотека поддерживает алгоритмы обучения как с учителем, так и без учителя. Вот список основных преимуществ данной библиотеки, делающих ее одной из самых предпочтительных библиотек Python для ML:\n",
        "\n",
        "снижение размерности\n",
        "алгоритмы, построенные на решающих деревьях (в том числе стрижка и индукция)\n",
        "построение решающих поверхностей\n",
        "анализ и выбор признаков\n",
        "обнаружение и удаление выбросов\n",
        "продвинутое вероятностное моделирование\n",
        "классификация и кластеризация без учителя"
      ],
      "metadata": {
        "id": "k5jSwF79hO8m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PyTorch** — это полностью готовая к работе библиотека машинного обучения Python с отличными примерами, приложениями и вариантами использования, поддерживаемая сильным сообществом. PyTorch отлично адаптирована к графическому процессору (GPU), что позволяет использовать его, например в приложениях NLP (обработка естественных языков). Вообще, поддержка вычислений на GPU и CPU обеспечивает оптимизацию и масштабирование распределенных задач обучения как в области исследований, так и в области создания ПО. Глубокие нейронные сети и тензорные вычисления с ускорением на GPU — две основные фишки PyTorch. Библиотека также включает в себя компилятор машинного обучения под названием Glow, который серьезно повышает производительность фреймворков глубокого обучения."
      ],
      "metadata": {
        "id": "tSGhFACjhVc-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SciPy**\n",
        "\n",
        "Она имеет различные модули для реализации машинного обучения. Особенность, которая делает библиотеку столь важной для машинного обучения, заключается в том, что она обеспечивает быстрое и качественное выполнение кода. Кроме того, это простая в использовании библиотека.\n",
        "\n",
        "Преимущества:\n",
        "Идеально подходит для обработки изображений.\n",
        "Предлагаются основные функции для осуществления математических операций.\n",
        "Обеспечивает эффективную интеграцию числовых данных и их оптимизацию.\n",
        "Также облегчает обработку сигналов.\n",
        "Недостатки:\n",
        "В общем-то, в использовании этой библиотеки нет серьезных недостатков. Однако может возникнуть путаница библиотеки SciPy с одноименным стеком, поскольку она в него включена."
      ],
      "metadata": {
        "id": "MzEWEgIDiP7a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Типы машинного обучения"
      ],
      "metadata": {
        "id": "2mgTx5f7hj1S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://pythonist.ru/mashinnoe-obuchenie-v-povsednevnoj-zhizni/"
      ],
      "metadata": {
        "id": "hJVttZHRh0u6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Введение в глубокое обучение: пошаговое руководство"
      ],
      "metadata": {
        "id": "nCXiD3Kpidc6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://pythonist.ru/vvedenie-v-glubokoe-obuchenie-po-shagam/?utm_source=telegram&utm_medium=pythonist"
      ],
      "metadata": {
        "id": "T61mAj7XifCe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Немного о Pandas"
      ],
      "metadata": {
        "id": "Zc_ZFoRPipeK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Скажем, вам нужно быстро проверить, есть ли в таблице значения NaN. В этом случае мы можем воспользоваться функцией count() , которая посчитает количество ячеек, содержащих какое-либо число.\n",
        "\n",
        "df.count()\n",
        "\n",
        "df.at[0,'price']= np.nan - Данная строка кода устанавливает значение NaN (Not a Number) в ячейку с индексом 0 в столбце 'price' объекта DataFrame df.\n",
        "\n",
        "Скажем, нам нужно посмотреть подробную информацию о доме с наименьшей ценой. Существует множество способов сделать это с помощью других методов. Но функции idxmin() и idxmax() наиболее эффективны.\n",
        "\n",
        "df.loc[df['price'].idxmin()]\n",
        "\n",
        "Допустим, у нас есть непрерывная переменная. Но, например, в рамках вашей задачи эту переменную необходимо рассматривать как категориальную.\n",
        "\n",
        "Функция cut() поможет вам привести непрерывную переменную к виду дискретной, разбив весь диапазон значений на интервалы.\n",
        "\n",
        "В нашем случае я хочу создать набор ценовых данных, поскольку значение цены колеблется от 0 до 26590000. Если я сгруппирую данные, с ними будет проще работать.\n",
        "\n",
        "pd.cut(df[\"price\"], 4)\n",
        "\n",
        "Если вы работали в excel, вы точно использовали эту функцию.\n",
        "\n",
        "Допустим, нам нужно найти среднюю цену дома в каждом городе, основываясь на количестве комнат.\n",
        "\n",
        "df.pivot_table(index=\"city\" , columns=\"bedrooms\" ,values=\"price\" , aggfunc=\"mean\")\n",
        "\n",
        "Мы уже научились использовать idxmin() и idxmax(), чтобы находить определённые значения. А что, если нужно найти три позиции с наибольшей ценой? Тут-то нам и пригодятся функции nsmallest() и nlargest().\n",
        "\n",
        "df.nlargest(3, \"price\")[[\"city\",\"price\"]]"
      ],
      "metadata": {
        "id": "kjPCFcOjitdN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Как очистить данные: пошаговое руководство"
      ],
      "metadata": {
        "id": "RpLZfZ6Ts8uL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# 1. Чтение данных из csv - способ по умолчанию\n",
        "df = pd.read_csv('my_file.csv')\n",
        "# 2. Чтение данных из csv с использованием запятой в качестве разделителя\n",
        "df = pd.read_csv('my_file.csv', delimiter=',')\n",
        "# 3. Чтение данных из csv с использованием запятой в качестве разделителя и без заголовков\n",
        "df = pd.read_csv('my_file.csv', delimiter=',', header=None)\n",
        "# 4. Чтение данных из csv с использованием запятой в качестве разделителя и с пользовательскими заголовками\n",
        "my_headers = ['Id','Name', 'Type', 'Price']\n",
        "df = pd.read_csv('my_file.csv', delimiter=',', header=0, names=my_headers)"
      ],
      "metadata": {
        "id": "hSrKm5OhtODe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Удаление дублирующихся данных"
      ],
      "metadata": {
        "id": "Rrzo7xkttAY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Удаление дубликатов и возврат копии датафрейма\n",
        "df = df.drop_duplicates()\n",
        "# 2. Удаление дубликатов в исходном датафрейме\n",
        "df = df.drop_duplicates(inplace=True)\n",
        "# 3. Отбрасываем дубликаты, оставляя первое/последнее вхождение\n",
        "df = df.drop_duplicates(inplace=True, keep='last')\n",
        "# 4. Для нахождения дубликатов учитываем только определенные столбцы\n",
        "df = df.drop_duplicates(subset=['Id', 'Price'], inplace=True, keep='last')"
      ],
      "metadata": {
        "id": "bFvH8G2FtSTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Перевод данных в нижний регистр"
      ],
      "metadata": {
        "id": "j5-FEfzrtX7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Type'] = df['Type'].str.lower()\n",
        "df['Name'] = df['Name'].str.lower()"
      ],
      "metadata": {
        "id": "ovEWZNxstYVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Удаление множественных пробелов, табов и символов перевода строки"
      ],
      "metadata": {
        "id": "pL92sE-UtaXU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Type'] = df['Type'].str.replace('\\n', '')\n",
        "df['Type'] = df['Type'].str.replace('\\t', ' ')\n",
        "df['Type'] = df['Type'].str.replace(' {2,}', ' ', regex=True)\n",
        "df['Type'] = df['Type'].str.strip()"
      ],
      "metadata": {
        "id": "ysicEr8htcnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Удаление URL-адресов"
      ],
      "metadata": {
        "id": "4OzANdQEthT_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Type'] = df['Type'].replace(r'http\\S+', '', regex=True).replace(r'www\\S+', '', regex=True)"
      ],
      "metadata": {
        "id": "IhiMCGEmtgrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Отбрасываем строки с пустыми данными"
      ],
      "metadata": {
        "id": "xLebt5-1t1iz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna()\n",
        "df['Type'].astype(bool)\n",
        "df = df[df['Type'].astype(bool)]"
      ],
      "metadata": {
        "id": "r8MIjriWt2Qi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Дальнейшая обработка данных"
      ],
      "metadata": {
        "id": "kKOImKICuAqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "df = df.drop(['Id', 'Name'], axis=1)\n",
        "df = df[df['Type'].str.contains('frozen') | df['Type'].str.contains('green')]\n",
        "def detect_price(row):\n",
        "    if row['Price'] > 15.50:\n",
        "        return 'High'\n",
        "    elif row['Price'] > 5.50 and row['Price'] <= 15.50:\n",
        "        return 'Medium'\n",
        "    elif row['Price'] > 0.0 and row['Price'] <= 5.50:\n",
        "        return 'Low'\n",
        "    else:\n",
        "        return np.NaN\n",
        "df['Range'] = df.apply (lambda row: detect_price(row), axis=1)"
      ],
      "metadata": {
        "id": "E4lJPHmwuEUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Здесь в третьей строке мы отбрасываем два столбца с именами Id и Name и возвращаем копию нового датафрейма.\n",
        "\n",
        "Четвертая строка проверяет, содержит ли столбец Type строку frozen или green, затем возвращает True и сохраняет эту строку.\n",
        "\n",
        "Строки с 7 по 17 создают новый столбец с именем Range на основе данных столбца Price. Используя лямбда-функцию, мы передаем каждую строку в функцию detect_price и возвращаем значение на основе цены. Затем возвращаемое значение присваивается новому столбцу в строке, переданной в функцию. Мы используем np.NaN, чтобы потом иметь возможность удалить эти строки при помощи df.dropna()."
      ],
      "metadata": {
        "id": "EbYUzBvfuGmH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6 тем"
      ],
      "metadata": {
        "id": "ypuyikEyjZMM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Прогнозирование кликов**"
      ],
      "metadata": {
        "id": "2a3UpRIyjb07"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для задачи прогнозирования кликов (click-through prediction) вы можете использовать различные методы машинного обучения, такие как линейная регрессия, случайные леса, градиентный бустинг и нейронные сети. Вот общий обзор шагов, которые можно выполнить для прогнозирования кликов:\n",
        "\n",
        "1. Подготовка данных: Загрузите и подготовьте данные. Это может включать в себя очистку данных, обработку пропущенных значений, кодирование категориальных признаков и масштабирование данных.\n",
        "\n",
        "2. Выбор признаков: Определите, какие признаки (факторы) будут использоваться для прогнозирования кликов. Это может включать в себя различные признаки, такие как демографическая информация, история кликов, контекстуальные данные и т.д.\n",
        "\n",
        "3. Разделение данных: Разделите данные на обучающий и тестовый наборы. Обычно используется метод кросс-валидации или разделение на обучающий и тестовый наборы в соотношении, например, 70% к 30%.\n",
        "\n",
        "4. Выбор модели: Выберите модель машинного обучения, которую хотите использовать для прогнозирования кликов. Это может быть линейная регрессия, случайный лес, градиентный бустинг и другие.\n",
        "\n",
        "5. Обучение модели: Обучите выбранную модель на обучающем наборе данных.\n",
        "\n",
        "6. Оценка модели: Оцените производительность модели на тестовом наборе данных, используя метрики, такие как точность, полнота, F1-мера, ROC-кривая и прочие.\n",
        "\n",
        "7. Настройка гиперпараметров: Проведите настройку гиперпараметров модели для улучшения ее производительности.\n",
        "\n",
        "8. Прогнозирование: Используйте обученную модель для прогнозирования кликов на новых данных.\n",
        "\n",
        "9. Оптимизация и масштабирование: При необходимости оптимизируйте и масштабируйте модель для улучшения ее производительности на реальных данных.\n",
        "\n",
        "Учитывая сложность задачи прогнозирования кликов, рекомендуется также изучить специализированные методы и подходы, которые используются в рекламной аналитике, такие как CTR (click-through rate) prediction и методы обработки больших объемов данных."
      ],
      "metadata": {
        "id": "iiOi48aCjkDE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для прогнозирования кликов в машинном обучении (ML) есть несколько популярных библиотек, которые можно использовать. Вот некоторые из них:\n",
        "\n",
        "scikit-learn: Это одна из наиболее популярных библиотек ML для Python. Она предоставляет широкий спектр алгоритмов для классификации, регрессии, кластеризации и других задач. Вы можете использовать модели, такие как логистическая регрессия, случайный лес, градиентный бустинг и другие для прогнозирования кликов.\n",
        "\n",
        "XGBoost: Это библиотека, специализирующаяся на градиентном бустинге деревьев решений. Она обеспечивает высокую производительность и эффективность при работе с большими объемами данных. XGBoost широко используется для решения задач классификации и регрессии.\n",
        "\n",
        "CatBoost: CatBoost является библиотекой градиентного бустинга, разработанной Яндексом. Она обладает специальными возможностями для работы с категориальными признаками и автоматическим выбором гиперпараметров. CatBoost имеет хорошую производительность и подходит для задач классификации, регрессии и ранжирования.\n",
        "\n",
        "TensorFlow: Это библиотека от Google, специализирующаяся на глубоком обучении. Она предоставляет высокоуровневые и низкоуровневые API для создания и обучения нейронных сетей. TensorFlow широко используется для задач классификации и регрессии, включая прогнозирование кликов.\n",
        "\n",
        "PyTorch: Это другая популярная библиотека глубокого обучения. Она обеспечивает гибкость и простоту в использовании. Подобно TensorFlow, PyTorch используется для создания и обучения нейронных сетей для задач прогнозирования кликов."
      ],
      "metadata": {
        "id": "zRYJjDOpjsM0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Снижение размерности данных**"
      ],
      "metadata": {
        "id": "81Evw3y_khV3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для снижения размерности данных в машинном обучении, вы можете использовать различные методы и библиотеки. Ниже приведены некоторые из наиболее часто используемых методов и библиотек для этой задачи:\n",
        "\n",
        "1. Метод главных компонент (PCA): PCA - это один из наиболее распространенных методов снижения размерности. Он позволяет найти линейные комбинации изначальных признаков, которые содержат наибольшее количество информации. В Python для реализации PCA можно использовать библиотеки scikit-learn или NumPy.\n",
        "\n",
        "2. t-distributed Stochastic Neighbor Embedding (t-SNE): t-SNE - это метод для визуализации и понижения размерности данных, который позволяет проецировать данные на пространство меньшей размерности. Библиотека scikit-learn предоставляет реализацию t-SNE.\n",
        "\n",
        "3. Linear Discriminant Analysis (LDA): LDA - это метод, который может быть использован для снижения размерности данных в задачах классификации. Он пытается найти комбинации признаков, которые максимизируют разделение классов. Реализация LDA также доступна в библиотеке scikit-learn.\n",
        "\n",
        "4. Autoencoders: Autoencoders - это нейронные сети, которые могут быть использованы для сжатия данных и снижения размерности. Библиотеки Keras и PyTorch предоставляют инструменты для реализации автоэнкодеров.\n",
        "\n",
        "5. Метод случайных проекций (Random Projection): Random Projection - это метод, который позволяет проецировать данные на случайно сгенерированные подпространства меньшей размерности. Этот метод может быть применен для снижения размерности данных. Реализации Random Projection также доступны в scikit-learn.\n",
        "\n",
        "6. Библиотеки: Для реализации снижения размерности данных в Python часто используются библиотеки scikit-learn, NumPy, pandas, Keras, PyTorch и TensorFlow.\n",
        "\n",
        "Это только несколько примеров методов и библиотек для снижения размерности данных. Выбор метода будет зависеть от специфики ваших данных и задачи."
      ],
      "metadata": {
        "id": "5zpP_3NPk-Ny"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Задачи работы с разреженными данными**"
      ],
      "metadata": {
        "id": "RpEL4s4ck_Bm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Работа с разреженными данными представляет собой важную задачу в области машинного обучения. Разреженные данные возникают, когда большинство элементов в наборе данных равны нулю. Это часто встречается в таких областях, как обработка естественного языка (NLP), рекомендательные системы, а также в задачах, связанных с большими разреженными матрицами (например, в рекомендательных системах).\n",
        "\n",
        "Ниже приведены некоторые задачи, связанные с работой с разреженными данными, а также методы и библиотеки, которые могут быть использованы для их решения:\n",
        "\n",
        "1. Представление разреженных данных: Эффективное представление и хранение разреженных данных является важной задачей. В Python для работы с разреженными матрицами можно использовать библиотеку scipy.sparse, которая предоставляет различные форматы для хранения разреженных матриц.\n",
        "\n",
        "2. Масштабирование и нормализация: При работе с разреженными данными важно учитывать их особенности при масштабировании и нормализации. scikit-learn предоставляет инструменты для масштабирования разреженных данных.\n",
        "\n",
        "3. Извлечение признаков: В некоторых случаях может потребоваться извлечение признаков из разреженных данных, например, при работе с текстовыми данными в NLP. Для этой задачи можно использовать методы векторизации текста, такие как TF-IDF или Word2Vec, а также библиотеку scikit-learn.\n",
        "\n",
        "4. Обучение моделей на разреженных данных: Многие модели машинного обучения могут быть обучены на разреженных данных, но некоторые модели, такие как модели глубокого обучения, могут требовать дополнительной предобработки данных. Библиотеки scikit-learn, TensorFlow, PyTorch и Keras предоставляют инструменты для обучения моделей на разреженных данных.\n",
        "\n",
        "5. Работа с рекомендательными системами: В рекомендательных системах часто используются разреженные матрицы для представления взаимодействия пользователей с элементами. Решение задач рекомендательных систем также требует специализированных методов и библиотек, таких как surprise, implicit, lightfm и другие.\n",
        "\n",
        "6. Оптимизация хранения данных: При работе с большими разреженными данными может потребоваться оптимизировать их хранение и обработку. Рассмотрите использование специализированных инструментов и библиотек для эффективной работы с разреженными данными.\n",
        "\n",
        "Работа с разреженными данными требует особого внимания к их уникальным особенностям и требует специализированных методов и инструментов для их эффективной обработки."
      ],
      "metadata": {
        "id": "CJhKk_vglbO3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Мультимодальное обучение**"
      ],
      "metadata": {
        "id": "eNJvLHoplcGL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Мультимодальное обучение относится к области машинного обучения, где информация из различных модальностей (таких как изображения, текст, звук и видео) используется вместе для решения задач анализа данных. Это важная область, так как в реальном мире информация обычно представлена не только в одной форме, а в нескольких модальностях одновременно.\n",
        "\n",
        "Ниже приведены некоторые примеры задач и методов, связанных с мультимодальным обучением:\n",
        "\n",
        "1. Обработка мультимодальных данных: Мультимодальные данные могут содержать изображения, текст, аудио, видео и другие типы информации. Для их обработки и представления могут быть использованы различные методы, такие как методы компьютерного зрения, обработки естественного языка (NLP), аудиообработки и другие.\n",
        "\n",
        "2. Обучение на мультимодальных данных: Для обучения моделей на мультимодальных данных могут использоваться различные методы, такие как объединение признаков из разных модальностей, использование общих представлений для разных модальностей, использование глубоких нейронных сетей и другие.\n",
        "\n",
        "3. Извлечение признаков из различных модальностей: Для извлечения признаков из различных модальностей можно использовать различные методы, такие как сверточные нейронные сети для изображений, рекуррентные нейронные сети для текста, а также методы извлечения признаков из аудио и видео.\n",
        "\n",
        "4. Объединение представлений: При решении задач мультимодального обучения важно научиться объединять информацию из разных модальностей для принятия решения. Это может включать в себя методы объединения признаков, мультимодальные архитектуры нейронных сетей и другие.\n",
        "\n",
        "5. Задачи мультимодального обучения: Примерами задач мультимодального обучения могут быть распознавание и классификация изображений сопровождаемые текстовыми описаниями, обработка и анализ медиа-контента (аудио и видео) с текстовыми и графическими данными, анализ социальных медиа и другие.\n",
        "\n",
        "Для решения задач мультимодального обучения могут использоваться различные библиотеки и фреймворки машинного обучения, такие как TensorFlow, PyTorch, scikit-learn и другие. Важно учитывать особенности каждой модальности и разрабатывать методы, способные эффективно работать с данными различных типов."
      ],
      "metadata": {
        "id": "pQYPXenzlr82"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Предобработка изображений**"
      ],
      "metadata": {
        "id": "0b7JjNj4ltBz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для выполнения этих шагов вы можете использовать различные библиотеки Python, такие как OpenCV, Pillow, scikit-image, а также специализированные библиотеки для глубокого обучения, такие как TensorFlow и PyTorch, которые предоставляют инструменты для предобработки изображений в рамках обучения моделей."
      ],
      "metadata": {
        "id": "yOtGBER5nWCZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Предобработка текста**"
      ],
      "metadata": {
        "id": "Ymdrfs4DnW16"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Предобработка текста играет важную роль в обработке данных для задач обработки естественного языка (Natural Language Processing, NLP) и других задач, связанных с анализом текстов. Вот несколько шагов, которые обычно включаются в процесс предобработки текста:\n",
        "\n",
        "1. Токенизация: Этот шаг включает разделение текста на отдельные слова или токены. Библиотеки такие как NLTK, spaCy или регулярные выражения могут быть использованы для выполнения токенизации.\n",
        "\n",
        "2. Преобразование в нижний регистр: Приведение всех слов к нижнему регистру может помочь унифицировать текст и избежать дублирования слов с разным регистром.\n",
        "\n",
        "3. Удаление стоп-слов: Стоп-слова (например, \"и\", \"в\", \"на\", \"не\") несут мало смысловой нагрузки, поэтому их удаление может улучшить производительность моделей. Многие библиотеки NLP, такие как NLTK, содержат списки стоп-слов для множества языков.\n",
        "\n",
        "4. Лемматизация или стемминг: Лемматизация и стемминг используются для приведения слов к их базовой форме. Лемматизация приводит слова к их лемме (например, \"бегал\", \"бежит\" -> \"бежать\"), в то время как стемминг обрезает слова до их основы (например, \"бегал\", \"бежит\" -> \"бег\"). Библиотеки NLTK и spaCy предоставляют инструменты для лемматизации и стемминга.\n",
        "\n",
        "5. Удаление специальных символов и цифр: В некоторых случаях может быть полезно удалить специальные символы, цифры и другие нежелательные символы из текста.\n",
        "\n",
        "6. Векторизация текста: После предобработки, текст должен быть преобразован в числовую форму, которую можно использовать в моделях машинного обучения. Это может быть достигнуто с использованием методов векторизации текста, таких как Bag of Words, TF-IDF или Word Embeddings.\n",
        "\n",
        "7. Аугментация данных: В некоторых случаях можно использовать методы аугментации текста для увеличения разнообразия данных, например, путем добавления синонимов, изменения порядка слов и других методов.\n",
        "\n",
        "Обычно для выполнения этих шагов используются различные библиотеки Python, такие как NLTK, spaCy, scikit-learn и другие, которые предоставляют инструменты для предобработки текста и работы с данными NLP. Выбор конкретных методов предобработки будет зависеть от задачи и особенностей ваших данных."
      ],
      "metadata": {
        "id": "wkLGH7-CoM_y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Фенички, которые были на кластере"
      ],
      "metadata": {
        "id": "-3J2SN_joZDZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Очистка памяти**"
      ],
      "metadata": {
        "id": "_nHHNyQpolCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "3ovFBG4mogIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Замер памяти**"
      ],
      "metadata": {
        "id": "3naV-EwDorVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install memory_profiler\n",
        "%load_ext memory_profiler\n",
        "%memit"
      ],
      "metadata": {
        "id": "p2ij5NgtoyOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Чтение файла**"
      ],
      "metadata": {
        "id": "RfdrhB9lpXhQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "DATASET_ROOT = \"../dataset\"\n",
        "TASK_DATASET = os.path.join(DATASET_ROOT, \"ok-group-recommend\")\n",
        "\n",
        "os.listdir(TASK_DATASET)"
      ],
      "metadata": {
        "id": "3sgG1wQopgG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "import csv"
      ],
      "metadata": {
        "id": "vOuEIW6apg7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X = pd.read_csv(os.path.join(TASK_DATASET, \"train_df.tsv.gz\"), delimiter=\"\\t\", compression='gzip')\n",
        "X = pd.read_csv(os.path.join(TASK_DATASET, \"train_df.tsv\"), delimiter=\"\\t\")"
      ],
      "metadata": {
        "id": "4c-5o293pjOU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}