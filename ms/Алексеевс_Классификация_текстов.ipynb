{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "В этом задании Вам предлагается решить проблему классификации текстов разными методами.\n",
        "\n",
        "Среди таких методов мы можем предложить Вам:\n",
        "\n",
        "1) Простой Байесовский классификатор на основе мультиномиальной модели или модели Бернулли\n",
        "\n",
        ">Достоинства: идейная простота и простота реализации, неплохая интерпретируемость\n",
        "\n",
        ">Недостатки: относительно слабая предсказательная способность\n",
        "\n",
        "> Frameworks: `numpy`\n",
        "\n",
        "2) Логистическая регрессия на основе векторов TF-IDF\n",
        "\n",
        ">Достоинства: достаточно высокая скорость обучения, простой метод составления эмбеддингов\n",
        "\n",
        ">Недостатки: также довольно слабая предсказательная способность, слишком высокая размерность задачи\n",
        "\n",
        "> Frameworks: `sklearn`, `numpy`\n",
        "\n",
        "3) Логистическая регрессия или нейронная сеть + word2vec embeddings\n",
        "\n",
        "> Достоинства: оптимальная размерность эмбеддингов, довольно простые модели, сравнительно неплохое качество\n",
        "\n",
        "> Недостатки: устаревший метод построения эмбеддингов. Эмбеддинги не контекстуальные\n",
        "\n",
        "> Frameworks: `gensim`, `pytorch`, `sklearn`\n",
        "\n",
        "4) Рекуррентная нейронная сеть + word2vec:\n",
        "\n",
        "> Достоинства: Более современная нейронная сеть\n",
        "\n",
        "> Недостатки: недоступно распараллеливание\n",
        "\n",
        "> Frameworks: `pytorch`, `gensim`\n",
        "\n",
        "5) ELMO + любая нейронная сеть\n",
        "\n",
        "> Достоинства: отличный контекстуальный метод векторизации текстов, мощная модель\n",
        "\n",
        "> Недостатки: сложность моделей\n",
        "\n",
        "> Frameworks: `elmo`, `pytorch`\n",
        "\n",
        "6) Bert + любая нейронная сеть\n",
        "\n",
        "> Достоинства: отличный контекстуальный метод векторизации текстов, мощная модель\n",
        "\n",
        "> Недостатки: сложность моделей\n",
        "\n",
        "> Frameworks: `transformers`, `pytorch`"
      ],
      "metadata": {
        "id": "WReIR-4NBULA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вы также можете исследовать любые комбинации методов векторизации и моделей ML, которые сочтете нужными.\n",
        "\n",
        "Ваша задача: провести сравнительный анализ не менее 3 алгоритмов классификации текстов. Сравнение стоит проводить по следующим параметрам:\n",
        "\n",
        "- Качество классификации (актуальную метрику выберите самостоятельно)\n",
        "- Время обучения модели\n",
        "- Характерное время инференса модели"
      ],
      "metadata": {
        "id": "NAy65KXJKuxL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Данные можно загрузить по ссылке: https://drive.google.com/drive/folders/14hR7Pm2sH28rQttkD906PTLvtwHFLBRm?usp=sharing"
      ],
      "metadata": {
        "id": "yfiXgUR6LSW0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для упрощения Вашей работы предлагаем ряд функций для предобработки текстов."
      ],
      "metadata": {
        "id": "2FTnQlZTL8yF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re, string\n",
        "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "def clear(text: str) -> str:\n",
        "    text = regex.sub('', text.lower())\n",
        "    text = re.sub(r'[«»\\n]', ' ', text)\n",
        "    text = text.replace('ё', 'е')\n",
        "    return text.strip()"
      ],
      "metadata": {
        "id": "BIpVS13MKtVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk #natural language toolkit\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "nltk.download('punkt') # без этой штуки мне выдает ошибку\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# проверяю, а то прикольно чет\n",
        "print(lemmatizer.lemmatize(\"people\"))\n",
        "print(lemmatizer.lemmatize(\"worst\"))\n",
        "print(lemmatizer.lemmatize(\"confused\"))\n",
        "print(lemmatizer.lemmatize(\"interacting\"))\n",
        "print(lemmatizer.lemmatize(\"campers\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXfcnWcvMKYg",
        "outputId": "22c5f3c3-3a79-4b4f-ee37-054068fd2ff6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "people\n",
            "worst\n",
            "confused\n",
            "interacting\n",
            "camper\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "eng_stopwords = stopwords.words(\"english\")\n",
        "\n",
        "remove_stopwords = lambda tokenized_text, stopwords: [w for w in tokenized_text if not w in stopwords]\n",
        "#type(remove_stopwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-QhsuQwMSkz",
        "outputId": "4d90944e-eb3c-4737-ef3d-ded034252bac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# $0)$ Предобработка данных"
      ],
      "metadata": {
        "id": "SwGwFBeqArbu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для начала загрузим датасет и предобработаем его. Проделаем это здесь, а потом будем использовать во всех трех решениях задачи.\n",
        "\n",
        "**Правда, я прописываю путь на свой гугл диск, так что с другого акканута, возможно, нужно прописать другой путь, чтобы файлы подгрузились. Не уверен.**"
      ],
      "metadata": {
        "id": "ykBznYx01HX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZTQ9mhnDqML",
        "outputId": "5fef87a9-5235-4e09-c2dd-6bdb34dcae94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/IMDB/\""
      ],
      "metadata": {
        "id": "fEd003nQNCqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts =[]\n",
        "train_labels = []\n",
        "\n",
        "test_texts =[]\n",
        "test_labels = []\n",
        "\n",
        "\n",
        "fp_train_texts = open(path+'train.texts','r',encoding='utf-8')\n",
        "for text in fp_train_texts:\n",
        "    train_texts.append(text)\n",
        "\n",
        "fp_train_labels = open(path+'train.labels','r',encoding='utf-8')\n",
        "for label in fp_train_labels:\n",
        "    train_labels.append(label)\n",
        "\n",
        "fp_test_texts = open(path+'dev.texts','r',encoding='utf-8')\n",
        "for text in fp_test_texts:\n",
        "    test_texts.append(text)\n",
        "\n",
        "fp_test_labels = open(path+'dev.labels','r',encoding='utf-8')\n",
        "for label in fp_test_labels:\n",
        "    test_labels.append(label)\n",
        "\n",
        "\n",
        "print('Длина тренировочного набора текстов: ', len(train_texts))\n",
        "print('Длина тестового набора текстов: ',len(test_texts))\n",
        "\n",
        "for i in range(5):\n",
        "  print(train_labels[i])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swQ4FL1SM_f1",
        "outputId": "11da0bc9-539c-4e58-fbcd-08b93a60c102"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Длина тренировочного набора текстов:  15000\n",
            "Длина тестового набора текстов:  10000\n",
            "neg\n",
            "\n",
            "pos\n",
            "\n",
            "neg\n",
            "\n",
            "neg\n",
            "\n",
            "pos\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# это просто проверка\n",
        "\n",
        "#train_texts[7]\n",
        "print(test_texts[5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qPd_sDoPokZ",
        "outputId": "e68a3ac9-6a88-4227-e174-d755e62afeca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This movie surprised me in a good way. From the box I got the impression that it was an action thriller but it was too funny to be a thriller, even though it was somewhat exciting.<br /><br />There's a lot of nice one-liners and funny situations in this movie and James Belushi was born to do Bill Manucci, he does a great job. The rest of the cast ain't half-bad either and especially Timothy Dalton is a treat.<br /><br />The story can get pretty confusing at times as new characters shows up during the film. Things get more complicated as nobody seldom tells the truth about things. If you don't pay attention things might get a bit messy in the end but I really liked it.<br /><br />Louis Morneau isn't all that well known but he has done a perfectly OK job with this one and I never really grew impatient while watching the movie.<br /><br />Made men is well worth checking out.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#предобработка текстов\n",
        "preprocessed_texts=[]\n",
        "preprocessed_test_texts=[]\n",
        "\n",
        "for text in train_texts:\n",
        "  text=clear(text)\n",
        "  #prepr=text.split() Вроде бы так неправильно\n",
        "  prepr=nltk.word_tokenize(text) # а так правильно\n",
        "  prepr=remove_stopwords(prepr,eng_stopwords)\n",
        "  for i in range(len(prepr)):\n",
        "    prepr[i]=lemmatizer.lemmatize(prepr[i])\n",
        "  preprocessed_texts.append(prepr)\n",
        "\n",
        "for text in test_texts:\n",
        "  text=clear(text)\n",
        "  #prepr=text.split() Вроде бы так неправильно\n",
        "  prepr=nltk.word_tokenize(text) # а так правильно\n",
        "  prepr=remove_stopwords(prepr,eng_stopwords)\n",
        "  for i in range(len(prepr)):\n",
        "    prepr[i]=lemmatizer.lemmatize(prepr[i])\n",
        "  preprocessed_test_texts.append(prepr)\n"
      ],
      "metadata": {
        "id": "lOKT6jNsRdkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts_train = [' '.join(t) for t in preprocessed_texts     ] # список уже полностью обработанных текстов\n",
        "texts_test =  [' '.join(t) for t in preprocessed_test_texts]\n",
        "print(preprocessed_texts[7],\"\\n\",\"\\n\")\n",
        "\n",
        "print(texts_test[5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-SuNGB5BouB",
        "outputId": "8788f2a1-44e4-40ff-8656-1a33b24a8f1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['say', 'worst', 'part', 'movie', 'first', 'half', 'hour', 'really', 'confused', 'example', 'bill', 'paxsons', 'character', 'long', 'hair', 'wearing', 'jacket', 'male', 'arrived', 'camp', 'turned', 'character', 'looked', 'like', 'bill', 'paxson', 'wasnt', 'said', 'wheres', 'bill', 'paxson', 'guy', 'girlfriend', 'said', '21', 'supposed', '20year', 'reunion', 'camp', 'director', 'alan', 'arkin', 'memorable', 'later', 'girl', 'interacting', 'talking', 'camp', 'experience', 'made', 'sense', 'would', 'one', 'year', 'old', 'said', 'movie', 'turned', 'pretty', 'good', 'kevin', 'pollak', 'nice', 'guy', 'always', 'teased', 'one', 'guy', 'complete', 'narcissist', 'ended', 'losing', 'beautiful', 'girlfriend', 'alan', 'arkin', 'interesting', 'oldstyle', 'camp', 'director', 'admits', 'grown', 'touch', 'modern', 'youth', 'best', 'part', 'none', 'grownup', 'camper', 'success', 'life', 'none', 'great', 'career', 'seemed', 'real', 'life', 'movie', 'compared', 'big', 'chill', 'way', 'wasnt', 'exciting', 'big', 'chill', 'lot', 'realistic', 'even', 'though', 'beginning', 'promising', 'movie', 'ended', 'turning', 'pretty', 'good', 'one'] \n",
            " \n",
            "\n",
            "movie surprised good way box got impression action thriller funny thriller even though somewhat excitingbr br there lot nice oneliners funny situation movie james belushi born bill manucci great job rest cast aint halfbad either especially timothy dalton treatbr br story get pretty confusing time new character show film thing get complicated nobody seldom tell truth thing dont pay attention thing might get bit messy end really liked itbr br louis morneau isnt well known done perfectly ok job one never really grew impatient watching moviebr br made men well worth checking\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Мы закончили с предобработкой данных."
      ],
      "metadata": {
        "id": "WDYPKPh3QH-7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# $1)$ Логистическая регрессия на основе векторов TF-IDF"
      ],
      "metadata": {
        "id": "pLfB6_EyMOEn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Векторизуем тексты."
      ],
      "metadata": {
        "id": "McdiGiVzJQhf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "X = vectorizer.fit_transform(texts_train) # Не понимаю, что именно делают эти 2 строчки\n",
        "X_test=vectorizer.transform(texts_test) # Я просто хочу векторизовать тексты и инет + ноутбук Сережи пишут, что это нужная команда\n",
        "\n",
        "print(type(X))\n",
        "print(len(texts_train))\n",
        "print(X.shape[0])\n",
        "print(X[0])\n"
      ],
      "metadata": {
        "id": "Hsr07f_oQ7pd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "293cfb1f-194d-45a5-9a56-9309a670d2f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'scipy.sparse.csr.csr_matrix'>\n",
            "15000\n",
            "15000\n",
            "  (0, 65795)\t0.08252149888958198\n",
            "  (0, 53973)\t0.015200485498484724\n",
            "  (0, 70969)\t0.0731920817366284\n",
            "  (0, 3928)\t0.06272649433070067\n",
            "  (0, 36209)\t0.05901703148157166\n",
            "  (0, 50475)\t0.02201718762556578\n",
            "  (0, 82274)\t0.06877682986582323\n",
            "  (0, 35783)\t0.06063193987869852\n",
            "  (0, 3557)\t0.06582448954444176\n",
            "  (0, 42468)\t0.06649144221267984\n",
            "  (0, 71627)\t0.06304347287285315\n",
            "  (0, 45575)\t0.0588068714783407\n",
            "  (0, 19200)\t0.043893172458207515\n",
            "  (0, 62896)\t0.07190123651327343\n",
            "  (0, 72903)\t0.03805186965155708\n",
            "  (0, 15951)\t0.05030914649319414\n",
            "  (0, 63477)\t0.04781279531056748\n",
            "  (0, 27710)\t0.05116598958330504\n",
            "  (0, 23959)\t0.03309674157259758\n",
            "  (0, 65858)\t0.043804686902994106\n",
            "  (0, 22354)\t0.02315694993614921\n",
            "  (0, 77834)\t0.053441728432244984\n",
            "  (0, 41677)\t0.05179931794142777\n",
            "  (0, 50986)\t0.08922213841353052\n",
            "  (0, 2529)\t0.02903989369740796\n",
            "  :\t:\n",
            "  (0, 65088)\t0.04650529723311677\n",
            "  (0, 28092)\t0.07494045177747681\n",
            "  (0, 10897)\t0.040832886323145676\n",
            "  (0, 31738)\t0.05556906214174497\n",
            "  (0, 67901)\t0.07860187603722199\n",
            "  (0, 66393)\t0.06299614628940686\n",
            "  (0, 57341)\t0.036210752570336686\n",
            "  (0, 44501)\t0.025470773748707643\n",
            "  (0, 26982)\t0.050395846441270375\n",
            "  (0, 45043)\t0.027857826052357003\n",
            "  (0, 45347)\t0.05492195954087575\n",
            "  (0, 6954)\t0.02387775362983395\n",
            "  (0, 84631)\t0.02569139903734731\n",
            "  (0, 344)\t0.09200315508511912\n",
            "  (0, 5287)\t0.07190123651327343\n",
            "  (0, 26772)\t0.03632181047433816\n",
            "  (0, 59616)\t0.03708205196702616\n",
            "  (0, 39402)\t0.04111215578661892\n",
            "  (0, 25847)\t0.05052737199958022\n",
            "  (0, 2202)\t0.053441728432244984\n",
            "  (0, 84102)\t0.06171114901588674\n",
            "  (0, 48941)\t0.22668223580395103\n",
            "  (0, 11044)\t0.27223497425970095\n",
            "  (0, 62336)\t0.11601828488258566\n",
            "  (0, 51018)\t0.06561201990569342\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучаем логистическую регрессию, делаем предсказание и чекаем его качество на train."
      ],
      "metadata": {
        "id": "umMlO4vHJVHp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "logisticRegr = LogisticRegression()\n",
        "logisticRegr.fit(X, train_labels)\n",
        "\n",
        "predictions = logisticRegr.predict(X_test)\n",
        "score = logisticRegr.score(X_test, test_labels)\n",
        "print(\"Accuracy:\",score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ya1ECyMQ4xL",
        "outputId": "cdb3725a-d0c6-425a-959c-8dc2e4daf8b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# $2)$ Простой Байесовский классификатор"
      ],
      "metadata": {
        "id": "_sWA6Yehbi3E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теория: просто записываем теорему Байеса, откидываем знаменатель, переписываем вероятность текста как произведение вероятностей слов и набрасываем логарифм на обе части (чтобы бороться с чем-то очень близким к нулю). Считаем это и делаем вывод о принадлежности какому-то классу.\n",
        "\n",
        "Но реализовывать уже сииииииииильно лень."
      ],
      "metadata": {
        "id": "rSKN5JJ3Mqe6"
      }
    }
  ]
}