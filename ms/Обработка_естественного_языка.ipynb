{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Обработка естественного языка\n",
        "\n",
        "В применении алгоритмов ML и DL к задачам обработки естественного языка существует своя специфика. Моделирование языка самого по себе - задача нетривиальная и имеет обычно вероятностную интерпретацию. Для работы с естественным языком, конкретизации возможных задач его обработки и методов их решения, зададим ряд специфических определений.\n",
        "\n",
        "**Определения:**\n",
        "\n",
        "**Def 1**: В рамках моделирования естественного языка принимается аксиома о существовании структурных единиц в рамках языка. Такие структурные единицы мы будем называть **термами** (terms). Обычно \"термы\" и \"слова\" - практически взаимозаменяемые понятия.\n",
        "\n",
        "**Def 2**: Тексты, предстовляющие из себя совокупность слов, часто называют **документами**, а набор текстов - **коллекцией**\n",
        "\n",
        "**Def 3**: В задачах определения некой характеристики текста особенно актуально понятие **стоп-слов**. Стоп-словом будем называть любой терм, вероятность появления которого в тексте близка к равномерной на множестве возможных характеристик текстов. Иначе говоря стоп-слова - это бесполезные для нас термы, которые мы можем встретить вообще в любом тексте примерно с одинаковой вероятностью, а значит, никаким образом этот текст не характеризующие. Хорошим примером стоп-слов являются служебные части речи, а в английском языке - артикли.\n",
        "\n",
        "**Def 4**: Стемминг - способ преобразования слова, в результате применения котрого у слова отбрасывается его окончание\n",
        "\n",
        "**Def 5**: Лемматизация - способ преобразования слова, в результате которого слово приводится к его начальной (или словарной) форме.\n"
      ],
      "metadata": {
        "id": "wQ_OxlcLJS6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk #natural language toolkit\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "word = \"dogs\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYwPNHt9NIF3",
        "outputId": "69837508-a31b-4fb7-939e-48cff092758b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1cOn2NDaOa1j",
        "outputId": "6ab0cf54-c2a0-41b3-9c4c-546c2743ae01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'dog'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Можно и на русском\n",
        "!pip install pymorphy2\n",
        "import pymorphy2\n",
        "\n",
        "morph = pymorphy2.MorphAnalyzer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a4GsEqjOg2V",
        "outputId": "f8008349-ed55-4595-bd5e-c7fac2a13d22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pymorphy2\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 KB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dawg-python>=0.7.1\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Collecting docopt>=0.6\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13721 sha256=a1016ac0730cd825ef998010f6d6dc71a9e8a41b7e047d95373bd78b8e729301\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/4a/46/1309fc853b8d395e60bafaf1b6df7845bdd82c95fd59dd8d2b\n",
            "Successfully built docopt\n",
            "Installing collected packages: pymorphy2-dicts-ru, docopt, dawg-python, pymorphy2\n",
            "Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word = 'Наикрасивейший'\n",
        "morph.parse(word)[0].normal_form"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SQnbZydpPeYt",
        "outputId": "9874d58b-21f9-40e7-dc07-c1a0ab391c7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'красивый'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Предобработка текстов\n",
        "\n",
        "Предобработка текстов - важнейший этап в решении любой задачи NLP. Предобработка текстов включает в себя следующие шаги:\n",
        "\n",
        "0. Очистка текста\n",
        "\n",
        "1. Токенизация текстов. Подразумевает разбиение текста на токены, то есть термы и их последовательности (униграммы, биграммы, триграммы и тд)\n",
        "\n",
        "2. Лемматизация/стемминг\n",
        "\n",
        "3. Удаление стоп-слов\n",
        "\n",
        "4. Выделение дополнительных признаков из текста\n",
        "\n",
        "5. Векторизация текста"
      ],
      "metadata": {
        "id": "ZRVyxQsuP3ax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import regexp_tokenize\n",
        "\n",
        "def tokenize_n_normalize(sent, pat=r\"(?u)\\b\\w\\w+\\b\", morph=pymorphy2.MorphAnalyzer()):\n",
        "    return [morph.parse(tok)[0].normal_form for tok in regexp_tokenize(sent, pat)]"
      ],
      "metadata": {
        "id": "fxS11MdxPvzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \\\n",
        "\"\"\"\n",
        "«Мой дядя самых честных правил,\n",
        "Когда не в шутку занемог,\n",
        "Он уважать себя заставил\n",
        "И лучше выдумать не мог.\n",
        "Его пример другим наука;\n",
        "Но, боже мой, какая скука\n",
        "С больным сидеть и день и ночь,\n",
        "Не отходя ни шагу прочь!\n",
        "Какое низкое коварство\n",
        "Полуживого забавлять,\n",
        "Ему подушки поправлять,\n",
        "Печально подносить лекарство,\n",
        "Вздыхать и думать про себя:\n",
        "Когда же черт возьмет тебя!»\n",
        "\n",
        "Так думал молодой повеса,\n",
        "Летя в пыли на почтовых,\n",
        "Всевышней волею Зевеса\n",
        "Наследник всех своих родных.\n",
        "Друзья Людмилы и Руслана!\n",
        "С героем моего романа\n",
        "Без предисловий, сей же час\n",
        "Позвольте познакомить вас:\n",
        "Онегин, добрый мой приятель,\n",
        "Родился на брегах Невы,\n",
        "Где, может быть, родились вы\n",
        "Или блистали, мой читатель;\n",
        "Там некогда гулял и я:\n",
        "Но вреден север для меня\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "VG9UOyhmUXgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re, string\n",
        "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "def clear(text: str) -> str:\n",
        "    text = regex.sub('', text.lower())\n",
        "    text = re.sub(r'[«»\\n]', ' ', text)\n",
        "    text = text.replace('ё', 'е')\n",
        "    return text.strip()"
      ],
      "metadata": {
        "id": "ZI-1dT_4VL8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clear(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "rX7eNmlqV64c",
        "outputId": "bf14ff4f-e6bb-40cf-bdd4-593a72ea9269"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'мой дядя самых честных правил когда не в шутку занемог он уважать себя заставил и лучше выдумать не мог его пример другим наука но боже мой какая скука с больным сидеть и день и ночь не отходя ни шагу прочь какое низкое коварство полуживого забавлять ему подушки поправлять печально подносить лекарство вздыхать и думать про себя когда же черт возьмет тебя   так думал молодой повеса летя в пыли на почтовых всевышней волею зевеса наследник всех своих родных друзья людмилы и руслана с героем моего романа без предисловий сей же час позвольте познакомить вас онегин добрый мой приятель родился на брегах невы где может быть родились вы или блистали мой читатель там некогда гулял и я но вреден север для меня'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clear_text = clear(text)\n",
        "tokenize_n_normalize(clear_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qK0mlZTYYbcy",
        "outputId": "1abe8b43-0901-4727-ac4f-5ae165f2cdfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['мой',\n",
              " 'дядя',\n",
              " 'самый',\n",
              " 'честной',\n",
              " 'правило',\n",
              " 'когда',\n",
              " 'не',\n",
              " 'шутка',\n",
              " 'занемочь',\n",
              " 'он',\n",
              " 'уважать',\n",
              " 'себя',\n",
              " 'заставить',\n",
              " 'хороший',\n",
              " 'выдумать',\n",
              " 'не',\n",
              " 'мочь',\n",
              " 'он',\n",
              " 'пример',\n",
              " 'другой',\n",
              " 'наука',\n",
              " 'но',\n",
              " 'бог',\n",
              " 'мой',\n",
              " 'какой',\n",
              " 'скука',\n",
              " 'больной',\n",
              " 'сидеть',\n",
              " 'день',\n",
              " 'ночь',\n",
              " 'не',\n",
              " 'отходить',\n",
              " 'ни',\n",
              " 'шаг',\n",
              " 'прочь',\n",
              " 'какой',\n",
              " 'низкий',\n",
              " 'коварство',\n",
              " 'полуживой',\n",
              " 'забавлять',\n",
              " 'он',\n",
              " 'подушка',\n",
              " 'поправлять',\n",
              " 'печально',\n",
              " 'подносить',\n",
              " 'лекарство',\n",
              " 'вздыхать',\n",
              " 'думать',\n",
              " 'про',\n",
              " 'себя',\n",
              " 'когда',\n",
              " 'же',\n",
              " 'черта',\n",
              " 'взять',\n",
              " 'ты',\n",
              " 'так',\n",
              " 'думать',\n",
              " 'молодой',\n",
              " 'повеса',\n",
              " 'лететь',\n",
              " 'пыль',\n",
              " 'на',\n",
              " 'почтовый',\n",
              " 'всевышний',\n",
              " 'воля',\n",
              " 'зевес',\n",
              " 'наследник',\n",
              " 'весь',\n",
              " 'свой',\n",
              " 'родный',\n",
              " 'друг',\n",
              " 'людмила',\n",
              " 'руслан',\n",
              " 'герой',\n",
              " 'мой',\n",
              " 'роман',\n",
              " 'без',\n",
              " 'предисловие',\n",
              " 'сей',\n",
              " 'же',\n",
              " 'час',\n",
              " 'позволить',\n",
              " 'познакомить',\n",
              " 'вы',\n",
              " 'онегин',\n",
              " 'добрый',\n",
              " 'мой',\n",
              " 'приятель',\n",
              " 'родиться',\n",
              " 'на',\n",
              " 'брег',\n",
              " 'нева',\n",
              " 'где',\n",
              " 'мочь',\n",
              " 'быть',\n",
              " 'родиться',\n",
              " 'вы',\n",
              " 'или',\n",
              " 'блистать',\n",
              " 'мой',\n",
              " 'читатель',\n",
              " 'там',\n",
              " 'некогда',\n",
              " 'гулять',\n",
              " 'но',\n",
              " 'вредный',\n",
              " 'север',\n",
              " 'для',\n",
              " 'я']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_text = tokenize_n_normalize(clear_text)\n",
        "\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "russian_stopwords = stopwords.words(\"russian\")\n",
        "len(russian_stopwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thIabb_6ZBJh",
        "outputId": "2964711f-d176-4620-f81b-020be575a602"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "151"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preproccessed_text = [w for w in tokenized_text if not w in russian_stopwords]\n",
        "preproccessed_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdI2KN5BZX5h",
        "outputId": "09caf864-e7bd-42d2-ec64-e7cc051a62d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['дядя',\n",
              " 'самый',\n",
              " 'честной',\n",
              " 'правило',\n",
              " 'шутка',\n",
              " 'занемочь',\n",
              " 'уважать',\n",
              " 'заставить',\n",
              " 'хороший',\n",
              " 'выдумать',\n",
              " 'мочь',\n",
              " 'пример',\n",
              " 'наука',\n",
              " 'бог',\n",
              " 'скука',\n",
              " 'больной',\n",
              " 'сидеть',\n",
              " 'день',\n",
              " 'ночь',\n",
              " 'отходить',\n",
              " 'шаг',\n",
              " 'прочь',\n",
              " 'низкий',\n",
              " 'коварство',\n",
              " 'полуживой',\n",
              " 'забавлять',\n",
              " 'подушка',\n",
              " 'поправлять',\n",
              " 'печально',\n",
              " 'подносить',\n",
              " 'лекарство',\n",
              " 'вздыхать',\n",
              " 'думать',\n",
              " 'черта',\n",
              " 'взять',\n",
              " 'думать',\n",
              " 'молодой',\n",
              " 'повеса',\n",
              " 'лететь',\n",
              " 'пыль',\n",
              " 'почтовый',\n",
              " 'всевышний',\n",
              " 'воля',\n",
              " 'зевес',\n",
              " 'наследник',\n",
              " 'весь',\n",
              " 'свой',\n",
              " 'родный',\n",
              " 'друг',\n",
              " 'людмила',\n",
              " 'руслан',\n",
              " 'герой',\n",
              " 'роман',\n",
              " 'предисловие',\n",
              " 'сей',\n",
              " 'час',\n",
              " 'позволить',\n",
              " 'познакомить',\n",
              " 'онегин',\n",
              " 'добрый',\n",
              " 'приятель',\n",
              " 'родиться',\n",
              " 'брег',\n",
              " 'нева',\n",
              " 'мочь',\n",
              " 'родиться',\n",
              " 'блистать',\n",
              " 'читатель',\n",
              " 'некогда',\n",
              " 'гулять',\n",
              " 'вредный',\n",
              " 'север']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text, stopwords):\n",
        "  clear_text = clear(text)\n",
        "  tokenized_text = tokenize_n_normalize(clear_text)\n",
        "  preproccessed_text = [w for w in tokenized_text if not w in stopwords]\n",
        "  return preproccessed_text"
      ],
      "metadata": {
        "id": "FUklxolvaEpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Решим задачу бинарной классификации отзывов о фильмах IMDB\n",
        "\n",
        "1) При помощи частотного подхода\n",
        "\n",
        "2) Обучив нейронную сеть"
      ],
      "metadata": {
        "id": "dRKoVxOHbc51"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"./drive/MyDrive/SummerSchool_2022/DATA/IMDB/\""
      ],
      "metadata": {
        "id": "h52ZN5JcbYDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts =[]\n",
        "train_labels = []\n",
        "\n",
        "test_texts =[]\n",
        "test_labels = []\n",
        "\n",
        "\n",
        "fp_train_texts = open(path+'train.texts','r',encoding='utf-8')\n",
        "for text in fp_train_texts:\n",
        "    train_texts.append(text)\n",
        "\n",
        "fp_train_labels = open(path+'train.labels','r',encoding='utf-8')\n",
        "for label in fp_train_labels:\n",
        "    train_labels.append(label)\n",
        "\n",
        "fp_test_texts = open(path+'dev.texts','r',encoding='utf-8')\n",
        "for text in fp_test_texts:\n",
        "    test_texts.append(text)\n",
        "\n",
        "fp_test_labels = open(path+'dev.labels','r',encoding='utf-8')\n",
        "for label in fp_test_labels:\n",
        "    test_labels.append(label)\n",
        "\n",
        "\n",
        "print('Длина тренировочного набора текстов: ', len(train_texts))\n",
        "print('Длина тестового набора текстов: ',len(test_texts))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdTDWAMOgDSK",
        "outputId": "002cb915-0f20-4ad4-a51a-2f3b370d46f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Длина тренировочного набора текстов:  15000\n",
            "Длина тестового набора текстов:  10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "from tqdm import tqdm\n",
        "stopwords_ = stopwords.words(\"english\")\n",
        "preprocessed_texts = []\n",
        "for text in tqdm(train_texts):\n",
        "  prep_text = preprocess(text, stopwords_)\n",
        "  preprocessed_texts.append(prep_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTdoQCHngcje",
        "outputId": "694e384a-3044-45c7-cd96-b2ed3e1ee674"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15000/15000 [01:25<00:00, 176.18it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = set([]) #Задаем словарь, как множество\n",
        "\n",
        "for text in tqdm(preprocessed_texts): #Заполняем его\n",
        "    vocab_of_text = set(text)\n",
        "    vocabulary = vocabulary.union(vocab_of_text)\n",
        "\n",
        "vocabulary = list(vocabulary) #преобразуем к list\n",
        "len(vocabulary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UY6ClpCsnznT",
        "outputId": "ba584add-0394-4f46-9940-20faf2c06b5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15000/15000 [00:39<00:00, 378.07it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "92749"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "negative_dict = {word:0 for word in vocabulary}\n",
        "positive_dict = negative_dict.copy()\n",
        "\n",
        "for i,text in tqdm(enumerate(preprocessed_texts)):\n",
        "    target = 0 if train_labels[i] == 'neg\\n' else 1\n",
        "    for word in text:\n",
        "        if target:\n",
        "            positive_dict[word]+=1\n",
        "        else:\n",
        "            negative_dict[word]+=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVscLHuCmgpK",
        "outputId": "481ebb3a-e61c-4b54-d4e3-5c5e7336e8cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "15000it [00:00, 25435.38it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Каждому слову припишем теперь некоторый \"ранг\", который определим как $rank(word) = \\frac{P-N}{T}$, где P = positive_dict[word], N = negative_dict[word], а T - это число, равное количеству нахождений слова word всего в текстах.\n",
        "\n",
        "При этом предлагается выкинуть те слова, которые встречаются совсем уж редко. Скажем, что не будем учитывать слова, которые встречаются менее 5 раз."
      ],
      "metadata": {
        "id": "rgghljZRokVB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rank_new = lambda word: rank(word) if word in vocabulary else 0\n",
        "decision = lambda text: 1 if sum([rank_new(word) for word in text]) > 0 else 0\n",
        "P = lambda word: positive_dict[word]\n",
        "N = lambda word: negative_dict[word]\n",
        "T = lambda word: positive_dict[word] + negative_dict[word]\n",
        "rank = lambda word: (P(word) - N(word))/(T(word))\n",
        "rank_dict = {word: 0 if T(word)<5 else rank(word) for word in vocabulary}"
      ],
      "metadata": {
        "id": "y2ltYtdcoW5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Предобработаем тестовые тексты"
      ],
      "metadata": {
        "id": "yWN7zOWyorfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_test_texts = []\n",
        "for text in tqdm(test_texts):\n",
        "  prep_text = preprocess(text, stopwords_)\n",
        "  preprocessed_test_texts.append(prep_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8co3D6mog-8",
        "outputId": "1b829018-30ba-4db4-d627-fd321fde6d8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [01:02<00:00, 159.07it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Замечание: Проверка всех 10000 текстов займет большое время выполнения. Поэтому мы выберем 500 случайных из них и оценим алгоритм на них"
      ],
      "metadata": {
        "id": "0f0b0rqQo68m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "set_of_indexes = np.arange(0,10000,1)\n",
        "indexes = set_of_indexes#np.random.choice(set_of_indexes,500, False)"
      ],
      "metadata": {
        "id": "IGLPVxDQoz8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_to_int = lambda label: 0 if label == 'neg\\n' else 1\n",
        "\n",
        "texts = [preprocessed_test_texts[j] for j in indexes]\n",
        "predictions = []\n",
        "y_true = []\n",
        "for i,text in tqdm(enumerate(texts)):\n",
        "    dec = decision(text)\n",
        "    label = label_to_int(test_labels[indexes[i]])\n",
        "    predictions.append(dec)\n",
        "    y_true.append(label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "o8QEN-YnpBA8",
        "outputId": "d9e8235e-178a-4cc5-b231-5a2da6be20cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "734it [02:26,  5.00it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-4f3a18ffa8ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mdec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_to_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindexes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-1dac1e3a0055>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrank_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocabulary\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrank_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpositive_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnegative_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpositive_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnegative_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-1dac1e3a0055>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrank_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocabulary\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrank_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpositive_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnegative_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpositive_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnegative_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-1dac1e3a0055>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(word)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrank_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocabulary\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrank_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpositive_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnegative_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpositive_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnegative_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy_score(y_true, predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJ-RESzgpn13",
        "outputId": "643e2c71-a4ea-4c97-b6c7-14d154a81a63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.85"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Применение моделей ML\n",
        "\n",
        "Результат неплох, но если мы хотим улучшить его с помощью применения алгоритмов машинного обучения и нейронных сетей, мы должны представить тексты в виде векторов. Существует множество способов сделать это.\n",
        "\n",
        "Начнем с разбора самых простых из этих методов."
      ],
      "metadata": {
        "id": "BY7-GGL3qVib"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. One-Hot encoding. Bag of Words\n",
        "\n",
        "Самый простой способ провести векторизацию текстов - это провести так называемое One-Hot преобразование.\n",
        "\n",
        "Оно состоит в следующем:\n",
        "Пусть мы имеем словарь, состоящий из N слов, каждое из которых имеет свой уникальный номер. Тогда слову w, имеющему в словаре номер i поставим в соответствие вектор [0 ... 0 1 0 ... 0], где 1 стоит на i-й позиции. Размерность такого вектора равна N.\n",
        "\n",
        "Пусть мы имеем документ D = {$w_1 ... w_m$}. Каждое из слов $w_j$ имеет своё векторное представление $\\vec{w_j}$, описанное выше. Тогда вектором документа D назовем векторную сумму $∑\\limits_{i=1}^{m}\\vec{w_i}$.\n",
        "\n",
        "Такой вектор имеет вид [0 ... 0 1 0 ... 0 1 0 ... 0 1 0 ... 0], где единицы стоят на местах, отвечающих номерам слов {$w_1 ... w_m$} в словаре\n",
        "\n",
        "# 2. TF-IDF encoding.\n",
        "\n",
        "Предыдущий способ векторизации имеет ряд недостатков:\n",
        "- Все слова в данном представлении абсолютно равнозначны по своим семантическим свойствам\n",
        "\n",
        "- Любые пары слов в таком представлении равноудалены друг от друга с точки зрения любой разумной метрики\n",
        "\n",
        "- Размерность полученного вектора получается слишком большой\n",
        "\n",
        "С последней проблемой мы будем бороться позже, а вот первые две попробуем исправить сейчас. Сделать это можно при помощи применения способа векторизации текстов, который называется tf-idf\n",
        "\n",
        "\n",
        "Идея этого метода состоит в том, что мы предполагаем существование некоторого скрытого параметра \"важности\" каждого слова в тексте для характеристики этого текста. Причем эта важность отвечает двум следующим условиям:\n",
        "\n",
        "1) Чем чаще слово встречается в корпусе текстов (по количеству документов, в котором оно встретилось), тем менее оно значимо для характеристики данного конкретного текста\n",
        "\n",
        "2) Чем чаще слово встречается непосредственно в самом рассматриваемом тексте, тем более оно значимо для его характеристики.\n",
        "\n",
        "Тогда важность эту можно выразить следующим образом: $$\\frac{TF}{DF}$$\n",
        "\n",
        "Где TF - Term Frequency - частота встречаемости данного слова в выбранном тексте,\n",
        "DF - Document Frequency - частота встречаемости слова в документах в рамках коллекции текстов.\n",
        "\n",
        "Иначе говоря, TF = $\\frac{N_d}{N}$, где $N_d$ - количество раз, которое слово w было встречено в документе d, а N - длина d в термах. DF = $\\frac{N_{df}}{M}$, где $N_{df}$ - количество текстов, в которых встретилось слово w, а M - мощность коллекции.\n",
        "\n",
        "Введем также величину inversed document frequency (IDF): $$IDF = ln(\\frac{1}{DF})$$\n",
        "Очевидно, IDF ~ $\\frac{1}{DF}$\n",
        "\n",
        "Тогда вес слова TF-IDF(word) = TF(word)*IDF(word)\n",
        "\n",
        "Тогда вектором документа D назовем векторную сумму $∑\\limits_{i=1}^{m}TF\\_IDF(w_i)\\vec{w_i}$\n",
        "\n",
        "К такому вектору уже применимы алгоритмы машинного обучения"
      ],
      "metadata": {
        "id": "9xT58NyarK2N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "v6iRzMzzt7vx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super(Net, self).__init__()\n",
        "        self.L1 = nn.Linear(dim, 1)\n",
        "        #self.L2 = nn.Linear(2000,100)\n",
        "        #self.L3 = nn.Linear(100,1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.L1(x)\n",
        "        #x = F.relu(x)\n",
        "        #x = self.L2(x)\n",
        "       # x = F.relu(x)\n",
        "       # x = self.L3(x)\n",
        "        x = torch.sigmoid(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "t2iAe8vKl7g8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "texts_train = [''.join(t) for t in preprocessed_texts     ]\n",
        "texts_test =  [''.join(t) for t in preprocessed_test_texts]\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "X = vectorizer.fit_transform(texts_train)"
      ],
      "metadata": {
        "id": "ar8GYNocnleY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "train_labels = np.array([1 if l == 'pos\\n' else 0 for l in train_labels])"
      ],
      "metadata": {
        "id": "W08MNH9gpn6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, x_val, y_train, y_val = train_test_split(X, train_labels, test_size=0.2, shuffle=True)"
      ],
      "metadata": {
        "id": "jUa6G3RXp5Kq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, X: np.array, Y: np.array):\n",
        "        super().__init__()\n",
        "        self.data = torch.FloatTensor(X)\n",
        "        self.target = torch.FloatTensor(Y)\n",
        "        self.data_shape = self.data.shape\n",
        "\n",
        "    def __getitem__(self,i):\n",
        "        return self.data[i,:], self.target[i]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data_shape[0]"
      ],
      "metadata": {
        "id": "FgIgN7AUog-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train = Dataset(X_train.toarray(), y_train)\n",
        "dataset_val   = Dataset(x_val.toarray(), y_val    )"
      ],
      "metadata": {
        "id": "7Ns3uRqJpXwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader_train = torch.utils.data.DataLoader(dataset_train,\n",
        "                                               batch_size=50,\n",
        "                                               shuffle=True,\n",
        "                                               num_workers=0)\n",
        "\n",
        "dataloader_val = torch.utils.data.DataLoader(dataset_val,\n",
        "                                               batch_size=3000,\n",
        "                                               shuffle=True,\n",
        "                                               num_workers=0)"
      ],
      "metadata": {
        "id": "yzylb70MqZnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "def train(model, dataloader_train, dataloader_val, num_epoch, log_every_n, optimizer, criterion):\n",
        "    step = 0\n",
        "    losses = []\n",
        "    accuracy = []\n",
        "    val_losses = []\n",
        "    val_accuracy = []\n",
        "    for t in range(num_epoch):\n",
        "        for X,y in dataloader_train:\n",
        "            y_pred = model(X)\n",
        "            loss = criterion(y_pred.view(-1), y.view(-1))\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            y_ans = (y_pred.detach().cpu().numpy() > 0.5).astype(int)\n",
        "            acc = accuracy_score(y_ans, y.detach().cpu().numpy())\n",
        "\n",
        "            losses.append(loss.item())\n",
        "            accuracy.append(acc)\n",
        "\n",
        "            if step % log_every_n == 0:\n",
        "              with torch.no_grad():\n",
        "                loss_val = []\n",
        "                acc_val = []\n",
        "                for X_val, y_val in dataloader_val:\n",
        "                    y_pred = model(X_val)\n",
        "                    loss_v = criterion(y_pred.view(-1), y_val.view(-1))\n",
        "                    loss_val.append(loss_v.item())\n",
        "\n",
        "                    y_ans_v = (y_pred.detach().cpu().numpy() > 0.5).astype(int)\n",
        "                    acc_v = accuracy_score(y_ans_v, y_val.detach().cpu().numpy())\n",
        "                    acc_val.append(acc_v)\n",
        "                loss_val = np.mean(loss_val)\n",
        "                val_losses.append(loss_val)\n",
        "                val_accuracy.append(np.mean(acc_val))\n",
        "            step+=1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return model, losses, accuracy, val_losses, val_accuracy\n"
      ],
      "metadata": {
        "id": "Ky0oGbgaqv-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dim = X_train.shape[1]\n",
        "NN = Net(dim=dim)\n",
        "\n",
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(NN.parameters(), lr=1e-3, weight_decay=1e-4)"
      ],
      "metadata": {
        "id": "RG851BPQuQPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NN, losses, accuracy, val_losses, val_accuracy = train(NN, dataloader_train, dataloader_val, num_epoch=3, log_every_n=10, optimizer=optimizer, criterion=criterion)"
      ],
      "metadata": {
        "id": "PE8IVwkq-MWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = torch.FloatTensor(vectorizer.transform(texts_test).toarray())\n",
        "test_labels = np.array([1 if l == 'pos\\n' else 0 for l in test_labels])\n",
        "\n",
        "preds_test = NN(X_test)\n",
        "answer = (preds_test.detach().numpy() > 0.5).astype(int)\n",
        "accuracy_score(answer, test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmYRB1R9uZwY",
        "outputId": "fb630151-9889-48a6-8f1d-76b8b9cc98ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.498"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yNPYPVBl02VX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}