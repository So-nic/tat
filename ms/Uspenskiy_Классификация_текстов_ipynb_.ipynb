{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Использовал туториал:\n",
        "https://towardsdatascience.com/text-classification-with-bert-in-pytorch-887965e5820f"
      ],
      "metadata": {
        "id": "_hxfXWU5l4IC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "В этом задании Вам предлагается решить проблему классификации текстов разными методами.\n",
        "\n",
        "Среди таких методов мы можем предложить Вам:\n",
        "\n",
        "1) Простой Байесовский классификатор на основе мультиномиальной модели или модели Бернулли\n",
        "\n",
        ">Достоинства: идейная простота и простота реализации, неплохая интерпретируемость\n",
        "\n",
        ">Недостатки: относительно слабая предсказательная способность\n",
        "\n",
        "> Frameworks: `numpy`\n",
        "\n",
        "2) Логистическая регрессия на основе векторов TF-IDF\n",
        "\n",
        ">Достоинства: достаточно высокая скорость обучения, простой метод составления эмбеддингов\n",
        "\n",
        ">Недостатки: также довольно слабая предсказательная способность, слишком высокая размерность задачи\n",
        "\n",
        "> Frameworks: `sklearn`, `numpy`\n",
        "\n",
        "3) Логистическая регрессия или нейронная сеть + word2vec embeddings\n",
        "\n",
        "> Достоинства: оптимальная размерность эмбеддингов, довольно простые модели, сравнительно неплохое качество\n",
        "\n",
        "> Недостатки: устаревший метод построения эмбеддингов. Эмбеддинги не контекстуальные\n",
        "\n",
        "> Frameworks: `gensim`, `pytorch`, `sklearn`\n",
        "\n",
        "4) Рекуррентная нейронная сеть + word2vec:\n",
        "\n",
        "> Достоинства: Более современная нейронная сеть\n",
        "\n",
        "> Недостатки: недоступно распараллеливание\n",
        "\n",
        "> Frameworks: `pytorch`, `gensim`\n",
        "\n",
        "5) ELMO + любая нейронная сеть\n",
        "\n",
        "> Достоинства: отличный контекстуальный метод векторизации текстов, мощная модель\n",
        "\n",
        "> Недостатки: сложность моделей\n",
        "\n",
        "> Frameworks: `elmo`, `pytorch`\n",
        "\n",
        "6) Bert + любая нейронная сеть\n",
        "\n",
        "> Достоинства: отличный контекстуальный метод векторизации текстов, мощная модель\n",
        "\n",
        "> Недостатки: сложность моделей\n",
        "\n",
        "> Frameworks: `transformers`, `pytorch`"
      ],
      "metadata": {
        "id": "WReIR-4NBULA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вы также можете исследовать любые комбинации методов векторизации и моделей ML, которые сочтете нужными.\n",
        "\n",
        "Ваша задача: провести сравнительный анализ не менее 3 алгоритмов классификации текстов. Сравнение стоит проводить по следующим параметрам:\n",
        "\n",
        "- Качество классификации (актуальную метрику выберите самостоятельно)\n",
        "- Время обучения модели\n",
        "- Характерное время инференса модели"
      ],
      "metadata": {
        "id": "NAy65KXJKuxL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Данные можно загрузить по ссылке: https://drive.google.com/drive/folders/14hR7Pm2sH28rQttkD906PTLvtwHFLBRm?usp=sharing"
      ],
      "metadata": {
        "id": "yfiXgUR6LSW0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для упрощения Вашей работы предлагаем ряд функций для предобработки текстов."
      ],
      "metadata": {
        "id": "2FTnQlZTL8yF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re, string\n",
        "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "def clear(text: str) -> str:\n",
        "    text = regex.sub('', text.lower())\n",
        "    text = re.sub(r'[«»\\n]', ' ', text)\n",
        "    text = text.replace('ё', 'е')\n",
        "    return text.strip()"
      ],
      "metadata": {
        "id": "BIpVS13MKtVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk #natural language toolkit\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXfcnWcvMKYg",
        "outputId": "49783086-4d1c-407a-92fe-fd725c12edb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "eng_stopwords = stopwords.words(\"english\")\n",
        "\n",
        "remove_stopwords = lambda tokenized_text, stopwords: [w for w in tokenized_text if not w in stopwords]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-QhsuQwMSkz",
        "outputId": "d1cd5fb7-dc34-4a8a-bf0a-5de04cf46cf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "WQZvdgjPM0KC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0337a613-4097-4631-9445-c242dbba5654"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy"
      ],
      "metadata": {
        "id": "JPd78uhUk7bK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# /content/drive/MyDrive/dev.labels\n",
        "# /content/drive/MyDrive/dev.texts\n",
        "# /content/drive/MyDrive/train.labels\n",
        "# /content/drive/MyDrive/train.texts\n"
      ],
      "metadata": {
        "id": "PaLK-ZBRN9t8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_file(path, if_text=False):\n",
        "  data = []\n",
        "  with open(path) as f:\n",
        "    if if_text:\n",
        "      for line in f:\n",
        "        data.append(' '.join(remove_stopwords(clear(line.strip('\\n')).split(' '), eng_stopwords)))\n",
        "    else:\n",
        "      for line in f:\n",
        "        data.append(line.strip())\n",
        "  return data\n",
        "\n",
        "test_text = read_file('/content/drive/MyDrive/dev.texts', True)\n",
        "test_labels = read_file('/content/drive/MyDrive/dev.labels')\n",
        "train_text = read_file('/content/drive/MyDrive/train.texts', True)\n",
        "train_labels = read_file('/content/drive/MyDrive/train.labels')"
      ],
      "metadata": {
        "id": "WxEghdRCOKln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({'category' : train_labels[:1000],\n",
        "                  'text' : train_text[:1000]})\n",
        "\n",
        "\n",
        "df_test = pd.DataFrame({'category' : test_labels[:1000],\n",
        "                  'text' : test_text[:1000]})\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "-pqhYBRzbcpj",
        "outputId": "b9106b7d-5859-4e75-a4a7-e8eb363b1afe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  category                                               text\n",
              "0      neg  myth regarding broken mirrors would accurate e...\n",
              "1      pos  gave movie 10 needed rewarded scary elements a...\n",
              "2      neg  watching first 20mn blanchesorry couldnt take ...\n",
              "3      neg  weak plot unlikely car malfunction helpless fu...\n",
              "4      pos  sidewalk ends 1950br br one ends another begin..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d3bba41f-c228-44d8-bcae-fdbdb49e77b0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>neg</td>\n",
              "      <td>myth regarding broken mirrors would accurate e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>pos</td>\n",
              "      <td>gave movie 10 needed rewarded scary elements a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>neg</td>\n",
              "      <td>watching first 20mn blanchesorry couldnt take ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>neg</td>\n",
              "      <td>weak plot unlikely car malfunction helpless fu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>pos</td>\n",
              "      <td>sidewalk ends 1950br br one ends another begin...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d3bba41f-c228-44d8-bcae-fdbdb49e77b0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d3bba41f-c228-44d8-bcae-fdbdb49e77b0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d3bba41f-c228-44d8-bcae-fdbdb49e77b0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bert-embedding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHcnamscYNLU",
        "outputId": "0d5ffd30-d43b-4710-c806-91f6f683fcc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: bert-embedding in /usr/local/lib/python3.7/dist-packages (1.0.1)\n",
            "Requirement already satisfied: mxnet==1.4.0 in /usr/local/lib/python3.7/dist-packages (from bert-embedding) (1.4.0)\n",
            "Requirement already satisfied: gluonnlp==0.6.0 in /usr/local/lib/python3.7/dist-packages (from bert-embedding) (0.6.0)\n",
            "Requirement already satisfied: typing==3.6.6 in /usr/local/lib/python3.7/dist-packages (from bert-embedding) (3.6.6)\n",
            "Requirement already satisfied: numpy==1.14.6 in /usr/local/lib/python3.7/dist-packages (from bert-embedding) (1.14.6)\n",
            "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from mxnet==1.4.0->bert-embedding) (0.8.4)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from mxnet==1.4.0->bert-embedding) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->mxnet==1.4.0->bert-embedding) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->mxnet==1.4.0->bert-embedding) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->mxnet==1.4.0->bert-embedding) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->mxnet==1.4.0->bert-embedding) (2022.6.15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "id": "I2CWVWSsZMRr",
        "outputId": "d68bdace-ba89-4162-dc25-1a42f43f3063"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.21.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.9.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Collecting numpy>=1.17\n",
            "  Using cached numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.14.6\n",
            "    Uninstalling numpy-1.14.6:\n",
            "      Successfully uninstalled numpy-1.14.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "mxnet 1.4.0 requires numpy<1.15.0,>=1.8.2, but you have numpy 1.21.6 which is incompatible.\n",
            "bert-embedding 1.0.1 requires numpy==1.14.6, but you have numpy 1.21.6 which is incompatible.\u001b[0m\n",
            "Successfully installed numpy-1.21.6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "example_text = 'I will watch Memento tonight'\n",
        "bert_input = tokenizer(example_text, padding='max_length', max_length = 10,\n",
        "                       truncation=True, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "IOMwp_4QVyhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_text = tokenizer.decode(bert_input.input_ids[0])\n",
        "\n",
        "print(example_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqPwpH9ZX8Ee",
        "outputId": "285f189e-04d0-4429-9df6-cad5f407d2a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS] I will watch Memento tonight [SEP] [PAD] [PAD]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "labels = {'neg':0,\n",
        "          'pos':1}\n",
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, df):\n",
        "\n",
        "        self.labels = [labels[label] for label in df['category']]\n",
        "        self.texts = [tokenizer(text,\n",
        "                               padding='max_length', max_length = 512, truncation=True,\n",
        "                                return_tensors=\"pt\") for text in df['text']]\n",
        "\n",
        "    def classes(self):\n",
        "        return self.labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def get_batch_labels(self, idx):\n",
        "        # Fetch a batch of labels\n",
        "        return np.array(self.labels[idx])\n",
        "\n",
        "    def get_batch_texts(self, idx):\n",
        "        # Fetch a batch of inputs\n",
        "        return self.texts[idx]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        batch_texts = self.get_batch_texts(idx)\n",
        "        batch_y = self.get_batch_labels(idx)\n",
        "\n",
        "        return batch_texts, batch_y"
      ],
      "metadata": {
        "id": "x6EwkcZndNTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from torch import nn\n",
        "from transformers import BertModel\n",
        "\n",
        "class BertClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, dropout=0.3):\n",
        "\n",
        "        super(BertClassifier, self).__init__()\n",
        "\n",
        "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear = nn.Linear(768, 2)\n",
        "        self.relu = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, input_id, mask):\n",
        "\n",
        "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
        "        dropout_output = self.dropout(pooled_output)\n",
        "        linear_output = self.linear(dropout_output)\n",
        "        final_layer = self.relu(linear_output)\n",
        "\n",
        "        return final_layer"
      ],
      "metadata": {
        "id": "CR_n_R-3aJzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train(model, train_data, val_data, learning_rate, epochs):\n",
        "\n",
        "    train, val = Dataset(train_data), Dataset(val_data)\n",
        "\n",
        "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=2, shuffle=True)\n",
        "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=2)\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = Adam(model.parameters(), lr= learning_rate)\n",
        "\n",
        "    if use_cuda:\n",
        "\n",
        "            model = model.cuda()\n",
        "            criterion = criterion.cuda()\n",
        "\n",
        "    for epoch_num in range(epochs):\n",
        "\n",
        "            total_acc_train = 0\n",
        "            total_loss_train = 0\n",
        "\n",
        "            for train_input, train_label in tqdm(train_dataloader):\n",
        "\n",
        "                train_label = train_label.to(device)\n",
        "\n",
        "                mask = train_input['attention_mask'].to(device)\n",
        "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "                output = model(input_id, mask)\n",
        "\n",
        "                batch_loss = criterion(output, train_label.long())\n",
        "                total_loss_train += batch_loss.item()\n",
        "\n",
        "                acc = (output.argmax(dim=1) == train_label).sum().item()\n",
        "                total_acc_train += acc\n",
        "\n",
        "                model.zero_grad()\n",
        "                batch_loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            total_acc_val = 0\n",
        "            total_loss_val = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "\n",
        "                for val_input, val_label in val_dataloader:\n",
        "\n",
        "                    val_label = val_label.to(device)\n",
        "                    mask = val_input['attention_mask'].to(device)\n",
        "                    input_id = val_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "                    output = model(input_id, mask)\n",
        "\n",
        "                    batch_loss = criterion(output, val_label.long())\n",
        "                    total_loss_val += batch_loss.item()\n",
        "\n",
        "                    acc = (output.argmax(dim=1) == val_label).sum().item()\n",
        "                    total_acc_val += acc\n",
        "\n",
        "            print(\n",
        "                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n",
        "                | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n",
        "                | Val Loss: {total_loss_val / len(val_data): .3f} \\\n",
        "                | Val Accuracy: {total_acc_val / len(val_data): .3f}')\n",
        "\n",
        "EPOCHS = 10\n",
        "model = BertClassifier()\n",
        "LR = 2e-3\n",
        "\n",
        "\n",
        "np.random.seed(112)\n",
        "df_train, df_val, _ = np.split(df.sample(frac=1, random_state=42),\n",
        "                                     [int(.8*len(df)), int(.9*len(df))])\n",
        "\n",
        "_, _, df_test = np.split(df_test.sample(frac=1, random_state=42),\n",
        "                                     [int(len(df_test)), int(0*len(df_test))])\n",
        "\n",
        "train(model, df_train, df_val, LR, EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k11VcpYnatSV",
        "outputId": "af6c3f2f-846b-4023-acf1-533cdd27c8ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "100%|██████████| 400/400 [01:22<00:00,  4.86it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 1 | Train Loss:  0.348                 | Train Accuracy:  0.510                 | Val Loss:  0.347                 | Val Accuracy:  0.470\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 400/400 [01:24<00:00,  4.74it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 2 | Train Loss:  0.347                 | Train Accuracy:  0.517                 | Val Loss:  0.347                 | Val Accuracy:  0.470\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 400/400 [01:25<00:00,  4.70it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 3 | Train Loss:  0.347                 | Train Accuracy:  0.519                 | Val Loss:  0.347                 | Val Accuracy:  0.470\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 400/400 [01:25<00:00,  4.70it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 4 | Train Loss:  0.347                 | Train Accuracy:  0.504                 | Val Loss:  0.347                 | Val Accuracy:  0.460\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 400/400 [01:25<00:00,  4.70it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 5 | Train Loss:  0.347                 | Train Accuracy:  0.521                 | Val Loss:  0.347                 | Val Accuracy:  0.470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 400/400 [01:24<00:00,  4.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 6 | Train Loss:  0.347                 | Train Accuracy:  0.509                 | Val Loss:  0.347                 | Val Accuracy:  0.470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 400/400 [01:25<00:00,  4.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 7 | Train Loss:  0.347                 | Train Accuracy:  0.515                 | Val Loss:  0.347                 | Val Accuracy:  0.470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 400/400 [01:25<00:00,  4.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 8 | Train Loss:  0.347                 | Train Accuracy:  0.511                 | Val Loss:  0.347                 | Val Accuracy:  0.470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 400/400 [01:24<00:00,  4.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 9 | Train Loss:  0.347                 | Train Accuracy:  0.516                 | Val Loss:  0.347                 | Val Accuracy:  0.530\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 400/400 [01:25<00:00,  4.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 10 | Train Loss:  0.347                 | Train Accuracy:  0.484                 | Val Loss:  0.347                 | Val Accuracy:  0.530\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, test_data):\n",
        "\n",
        "    test = Dataset(test_data)\n",
        "\n",
        "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=2)\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    if use_cuda:\n",
        "\n",
        "        model = model.cuda()\n",
        "\n",
        "    total_acc_test = 0\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for test_input, test_label in test_dataloader:\n",
        "\n",
        "              test_label = test_label.to(device)\n",
        "              mask = test_input['attention_mask'].to(device)\n",
        "              input_id = test_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "              output = model(input_id, mask)\n",
        "\n",
        "              acc = (output.argmax(dim=1) == test_label).sum().item()\n",
        "              total_acc_test += acc\n",
        "\n",
        "    print(f'Test Accuracy: {total_acc_test / len(test_data): .3f}')\n",
        "\n",
        "evaluate(model, df_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nf2o6Ohwrx87",
        "outputId": "b7d11eff-2ebb-466b-ccc9-76cd6506672a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy:  0.518\n"
          ]
        }
      ]
    }
  ]
}